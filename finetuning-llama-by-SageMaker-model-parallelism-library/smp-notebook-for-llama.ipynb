{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training Demo for llama on SageMaker\n",
    "\n",
    "### Model Parallelism using SageMaker model parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Development Environment and Permissions](#Development-Environment-and-Permissions)\n",
    "    1. [Installation](#Installation)  \n",
    "    2. [Development environment](#Development-environment)  \n",
    "    3. [Permissions](#Permissions)\n",
    "3. [Processing](#Preprocessing)   \n",
    "    1. [Tokenization](#Tokenization)  \n",
    "    2. [Uploading data to sagemaker_session_bucket](#Uploading-data-to-sagemaker_session_bucket)  \n",
    "4. [Fine-tuning & starting Sagemaker Training Job](#Fine-tuning-\\&-starting-Sagemaker-Training-Job)  \n",
    "    1. [Creating an Estimator and start a training job](#Creating-an-Estimator-and-start-a-training-job)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end distributed training example. In this demo, we will use the Hugging Face `transformers` and `datasets` library together with a Amazon sagemaker-sdk extension on a multi-node multi-gpu cluster using [SageMaker Model Parallelism Library](https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-intro.html). The demo will use the new smdistributed library to run training on multiple gpus. \n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio, your local machine or Sagemaker Notebook Instances**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you havenÂ´t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker>=2.156.0 in /opt/conda/lib/python3.7/site-packages (2.156.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (23.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.0.1)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.20.3)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.7.5)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (4.13.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.21.6)\n",
      "Requirement already satisfied: PyYAML==6.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (6.0)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.1.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (0.3.0)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (3.2.0)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (23.1.0)\n",
      "Requirement already satisfied: tblib==1.7.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.7.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.131 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.26.135)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.156.0) (1.3.5)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.135 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.29.135)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.156.0) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.156.0) (3.15.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.156.0) (1.14.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.156.0) (0.15.7)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from jsonschema->sagemaker>=2.156.0) (59.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.156.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.156.0) (2019.3)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (1.7.6.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.3.2)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.156.0) (0.3.6)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker>=2.156.0) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.135->boto3<2.0,>=1.26.131->sagemaker>=2.156.0) (1.26.15)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.156.0\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After upgrading the sagemaker sdk library, please restart the jupyter kernel and execute the following cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker.huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.156.0\n"
     ]
    }
   ],
   "source": [
    "print(sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::514385905925:role/service-role/AmazonSageMaker-ExecutionRole-20201218T184365\n",
      "sagemaker bucket: sagemaker-us-east-1-514385905925\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the training container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "The `hyperparameters` you define in the estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimatorâs fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "#hyperparameters for llama\n",
    "hyperparameters={\n",
    "  'training_dir': '/opt/ml/input/data/train', # path where sagemaker will save training dataset\n",
    "  'test_dir': '/opt/ml/input/data/test',      # path where sagemaker will save test dataset\n",
    "  'num_train_epochs': 1,                                         # number of training epochs\n",
    "  'per_device_train_batch_size': 2,                    # batch size for training\n",
    "  'per_device_eval_batch_size': 2,                     # batch size for evaluation\n",
    "  'learning_rate': 1e-5,   \n",
    "  'gradient_accumulation_steps': 4,\n",
    "  'model_max_length': 1536                          # learning rate used during training\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Model Parallel\n",
    "mpi_options = {\n",
    "    \"enabled\" : True,\n",
    "    \"processes_per_host\" : 8,\n",
    "}\n",
    "\n",
    "smp_options = {\n",
    "    \"enabled\":True,\n",
    "    \"parameters\": {\n",
    "        \"pipeline_parallel_degree\": 16,\n",
    "        \"placement_strategy\": \"cluster\",\n",
    "        \"tensor_parallel_degree\": 1,\n",
    "        \"partitions\": 16,\n",
    "        \"fp16\": True,\n",
    "        \"ddp\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "distribution={\n",
    "    \"smdistributed\": {\"modelparallel\": smp_options},\n",
    "    \"mpi\": mpi_options\n",
    "}\n",
    "\n",
    "# instance configurations\n",
    "instance_type='ml.p4d.24xlarge'\n",
    "instance_count = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# estimator\n",
    "#define the model s3 path which will store your trained model asset\n",
    "#Note: you should use your real s3 path to configure model_s3_path\n",
    "target_model_s3_path='s3://your_bucket/llama-smp-finetuned-052111/model/'\n",
    "\n",
    "#define the s3 path of source model before training.  \n",
    "#Note: Please add the wildcard character '*' in the following path, otherwise error will happen.\n",
    "source_model_s3_path = 's3://your_bucket/llama/pretrained/7B/model/*'\n",
    "\n",
    "environment = {'CUDA_LAUNCH_BLOCKING': '1',\n",
    "               'SOURCE_MODEL_BEFORE_TRAINING_S3_PATH': source_model_s3_path,\n",
    "               'TARGET_MODEL_AFTER_TRAINING_S3_PATH': target_model_s3_path}\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "'''\n",
    "huggingface_estimator = HuggingFace(entry_point='train-llama-file-lock-for-HF-container.py',\n",
    "                                    source_dir           = '.', \n",
    "                                    instance_type=instance_type,\n",
    "                                    instance_count=instance_count,\n",
    "                                    role=role,\n",
    "                                    transformers_version='4.17',\n",
    "                                    pytorch_version='1.10',\n",
    "                                    py_version='py38',\n",
    "                                    distribution= distribution,\n",
    "                                    hyperparameters = hyperparameters,\n",
    "                                    environment = environment,\n",
    "                                    debugger_hook_config=False)\n",
    "'''\n",
    "\n",
    "huggingface_estimator = PyTorch(entry_point='train-llama-for-pytorch-container.py',\n",
    "                                source_dir           = '.', \n",
    "                                instance_type=instance_type,\n",
    "                                instance_count=instance_count,\n",
    "                                role=role,\n",
    "                                framework_version='1.12.0',\n",
    "                                py_version='py38',\n",
    "                                distribution= distribution,\n",
    "                                hyperparameters = hyperparameters,\n",
    "                                environment = environment,\n",
    "                                debugger_hook_config=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_dir': '\"/opt/ml/input/data/train\"',\n",
       " 'test_dir': '\"/opt/ml/input/data/test\"',\n",
       " 'num_train_epochs': '1',\n",
       " 'per_device_train_batch_size': '2',\n",
       " 'per_device_eval_batch_size': '2',\n",
       " 'learning_rate': '1e-05',\n",
       " 'gradient_accumulation_steps': '4',\n",
       " 'model_max_length': '1536',\n",
       " 'sagemaker_mpi_enabled': 'true',\n",
       " 'sagemaker_mpi_num_of_processes_per_host': '8',\n",
       " 'sagemaker_mpi_custom_mpi_options': '\"\"',\n",
       " 'mp_parameters': '{\"pipeline_parallel_degree\": 16, \"placement_strategy\": \"cluster\", \"tensor_parallel_degree\": 1, \"partitions\": 16, \"fp16\": true, \"ddp\": true}',\n",
       " 'sagemaker_distributed_dataparallel_enabled': 'false',\n",
       " 'sagemaker_instance_type': '\"ml.p4d.24xlarge\"'}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-05-21-03-28-47-224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-21 03:29:55 Starting - Starting the training job......\n",
      "2023-05-21 03:30:34 Starting - Preparing the instances for training.....................\n",
      "2023-05-21 03:34:23 Downloading - Downloading input data...\n",
      "2023-05-21 03:34:48 Training - Downloading the training image..................\n",
      "2023-05-21 03:37:29 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,041 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,111 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:25,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,445 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,516 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:25,518 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:29,345 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-99026zqk/transformers_429a703eaa7a427bae1d734b18480741\u001b[0m\n",
      "\u001b[34mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-99026zqk/transformers_429a703eaa7a427bae1d734b18480741\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:29,530 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.8 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting transformers@ git+https://github.com/huggingface/transformers.git@97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mCloning https://github.com/huggingface/transformers.git (to revision 97a3d16a6941294d7d76d24f36f26617d224278e) to /tmp/pip-install-ijim08u_/transformers_75a17121a0ab41df9a4cf4f08230c715\u001b[0m\n",
      "\u001b[35mRunning command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-ijim08u_/transformers_75a17121a0ab41df9a4cf4f08230c715\u001b[0m\n",
      "\u001b[34mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[34mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mRunning command git rev-parse -q --verify 'sha^97a3d16a6941294d7d76d24f36f26617d224278e'\u001b[0m\n",
      "\u001b[35mRunning command git fetch -q https://github.com/huggingface/transformers.git 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mRunning command git checkout -q 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[35mResolved https://github.com/huggingface/transformers.git to commit 97a3d16a6941294d7d76d24f36f26617d224278e\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[35mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 469.0/469.0 kB 36.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[34mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 81.4/81.4 kB 23.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.10.2.3)\u001b[0m\n",
      "\u001b[34mCollecting rouge-score\u001b[0m\n",
      "\u001b[34mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 1.3/1.3 MB 69.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[34mCollecting responses<0.19\u001b[0m\n",
      "\u001b[34mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[34mCollecting xxhash\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 213.0/213.0 kB 42.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[35mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting datasets==2.10.1\u001b[0m\n",
      "\u001b[35mDownloading datasets-2.10.1-py3-none-any.whl (469 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 469.0/469.0 kB 36.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting evaluate==0.4.0\u001b[0m\n",
      "\u001b[35mDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââââ 81.4/81.4 kB 22.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.10.2.3)\u001b[0m\n",
      "\u001b[35mCollecting rouge-score\u001b[0m\n",
      "\u001b[35mDownloading rouge_score-0.1.2.tar.gz (17 kB)\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[35mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35mDownloading sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââââ 1.3/1.3 MB 85.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.70.13)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.4.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (5.4.1)\u001b[0m\n",
      "\u001b[35mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[35mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 224.5/224.5 kB 48.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (4.64.0)\u001b[0m\n",
      "\u001b[35mCollecting responses<0.19\u001b[0m\n",
      "\u001b[35mDownloading responses-0.18.0-py3-none-any.whl (38 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (9.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2.28.1)\u001b[0m\n",
      "\u001b[35mCollecting xxhash\u001b[0m\n",
      "\u001b[35mDownloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 213.0/213.0 kB 43.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting aiohttp\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 1.0/1.0 MB 77.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.7,>=0.3.0 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (0.3.5.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0.0,>=0.2.0\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 224.5/224.5 kB 39.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.4.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (2022.7.1)\u001b[0m\n",
      "\u001b[35mCollecting aiohttp\u001b[0m\n",
      "\u001b[35mDownloading aiohttp-3.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââââ 1.0/1.0 MB 89.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (1.22.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from datasets==2.10.1->-r requirements.txt (line 2)) (21.3)\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35mDownloading filelock-3.12.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[35mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[35mDownloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââââ 7.8/7.8 MB 105.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[35mDownloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 771.9/771.9 kB 74.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting absl-py\u001b[0m\n",
      "\u001b[35mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 126.5/126.5 kB 33.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting nltk\u001b[0m\n",
      "\u001b[35mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââââ 1.5/1.5 MB 97.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[35mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[35mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 121.3/121.3 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[35mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[35mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 161.3/161.3 kB 40.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[35mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[35mDownloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\u001b[0m\n",
      "\u001b[35mââââââââââââââââââââââââââââââââââââââ 266.9/266.9 kB 55.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[35mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[35mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: transformers, rouge-score\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=38c9e09fac67debf31b0ce55bfed6de023b856a31c2ff86a3622da36daec23d9\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=05bc249bb283370bf4c6580ccc8787ed013649d7f4610eafd3eaef7d42564884\u001b[0m\n",
      "\u001b[35mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[35mSuccessfully built transformers rouge-score\u001b[0m\n",
      "\u001b[35mInstalling collected packages: tokenizers, sentencepiece, xxhash, regex, multidict, frozenlist, filelock, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge-score, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.5.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 771.9/771.9 kB 70.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34mDownloading filelock-3.12.0-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 7.8/7.8 MB 108.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting absl-py\u001b[0m\n",
      "\u001b[34mDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 126.5/126.5 kB 28.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting nltk\u001b[0m\n",
      "\u001b[34mDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââââ 1.5/1.5 MB 96.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from rouge-score->-r requirements.txt (line 5)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.10.1->-r requirements.txt (line 2)) (4.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->datasets==2.10.1->-r requirements.txt (line 2)) (3.0.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (2022.6.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (1.26.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->datasets==2.10.1->-r requirements.txt (line 2)) (3.3)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0.0a3\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets==2.10.1->-r requirements.txt (line 2)) (21.4.0)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 121.3/121.3 kB 24.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.3.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 161.3/161.3 kB 38.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.0\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\u001b[0m\n",
      "\u001b[34mââââââââââââââââââââââââââââââââââââââ 266.9/266.9 kB 48.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (8.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->rouge-score->-r requirements.txt (line 5)) (1.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets==2.10.1->-r requirements.txt (line 2)) (2022.2.1)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: transformers, rouge-score\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for transformers (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for transformers: filename=transformers-4.28.0.dev0-py3-none-any.whl size=6756543 sha256=0b318410909e1291765d89f768165ce3d8614d80cd167d6ea26a90f385f783ba\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/5c/1d/64/c9c142433b568c604e359bd903fe1424c3707274fecc075ace\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for rouge-score (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24936 sha256=05bc249bb283370bf4c6580ccc8787ed013649d7f4610eafd3eaef7d42564884\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\u001b[0m\n",
      "\u001b[34mSuccessfully built transformers rouge-score\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, sentencepiece, xxhash, regex, multidict, frozenlist, filelock, async-timeout, absl-py, yarl, responses, nltk, huggingface-hub, aiosignal, transformers, rouge-score, aiohttp, datasets, evaluate\u001b[0m\n",
      "\u001b[34mSuccessfully installed absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 nltk-3.8.1 regex-2023.5.5 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.0.dev0 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,166 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,166 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,307 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,307 sagemaker-training-toolkit INFO     Creating SSH daemon.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,311 sagemaker-training-toolkit INFO     Waiting for MPI workers to establish their SSH connections\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,318 sagemaker-training-toolkit INFO     Cannot connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:53,318 sagemaker-training-toolkit INFO     Connection failed with exception: \n",
      " [Errno None] Unable to connect to port 22 on 10.0.209.56\u001b[0m\n",
      "\u001b[35mSuccessfully installed absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 evaluate-0.4.0 filelock-3.12.0 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 nltk-3.8.1 regex-2023.5.5 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.0.dev0 xxhash-3.2.0 yarl-1.9.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip available: 22.2.2 -> 23.1.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,477 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,477 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,623 sagemaker-training-toolkit INFO     Starting MPI run as worker node.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,624 sagemaker-training-toolkit INFO     Waiting for MPI Master to create SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,634 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,695 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     Can connect to host algo-1\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     MPI Master online, creating SSH daemon.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,696 sagemaker-training-toolkit INFO     Writing environment variables to /etc/environment for the MPI process.\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:53,702 sagemaker-training-toolkit INFO     Waiting for MPI process to finish.\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,328 paramiko.transport INFO     Connected (version 2.0, client OpenSSH_8.2p1)\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 paramiko.transport INFO     Authentication (publickey) successful!\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Can connect to host algo-2\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Worker algo-2 available for communication\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,392 sagemaker-training-toolkit INFO     Env Hosts: ['algo-1', 'algo-2'] Hosts: ['algo-1:8', 'algo-2:8'] process_per_hosts: 8 num_processes: 16\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,394 sagemaker-training-toolkit INFO     Network interface name: eth0\u001b[0m\n",
      "\u001b[34m2023-05-21 03:38:54,468 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_distributed_dataparallel_enabled\": false,\n",
      "        \"sagemaker_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"sagemaker_mpi_custom_mpi_options\": \"\",\n",
      "        \"sagemaker_mpi_enabled\": true,\n",
      "        \"sagemaker_mpi_num_of_processes_per_host\": 8\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"gradient_accumulation_steps\": 4,\n",
      "        \"learning_rate\": 1e-05,\n",
      "        \"model_max_length\": 1536,\n",
      "        \"mp_parameters\": {\n",
      "            \"pipeline_parallel_degree\": 16,\n",
      "            \"placement_strategy\": \"cluster\",\n",
      "            \"tensor_parallel_degree\": 1,\n",
      "            \"partitions\": 16,\n",
      "            \"fp16\": true,\n",
      "            \"ddp\": true\n",
      "        },\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"per_device_eval_batch_size\": 2,\n",
      "        \"per_device_train_batch_size\": 2,\n",
      "        \"test_dir\": \"/opt/ml/input/data/test\",\n",
      "        \"training_dir\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": true,\n",
      "    \"job_name\": \"pytorch-training-2023-05-21-03-28-47-224\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train-llama-for-pytorch-container\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train-llama-for-pytorch-container.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"gradient_accumulation_steps\":4,\"learning_rate\":1e-05,\"model_max_length\":1536,\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train-llama-for-pytorch-container.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train-llama-for-pytorch-container\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_distributed_dataparallel_enabled\":false,\"sagemaker_instance_type\":\"ml.p4d.24xlarge\",\"sagemaker_mpi_custom_mpi_options\":\"\",\"sagemaker_mpi_enabled\":true,\"sagemaker_mpi_num_of_processes_per_host\":8},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"gradient_accumulation_steps\":4,\"learning_rate\":1e-05,\"model_max_length\":1536,\"mp_parameters\":{\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1},\"num_train_epochs\":1,\"per_device_eval_batch_size\":2,\"per_device_train_batch_size\":2,\"test_dir\":\"/opt/ml/input/data/test\",\"training_dir\":\"/opt/ml/input/data/train\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"job_name\":\"pytorch-training-2023-05-21-03-28-47-224\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-514385905925/pytorch-training-2023-05-21-03-28-47-224/source/sourcedir.tar.gz\",\"module_name\":\"train-llama-for-pytorch-container\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train-llama-for-pytorch-container.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--gradient_accumulation_steps\",\"4\",\"--learning_rate\",\"1e-05\",\"--model_max_length\",\"1536\",\"--mp_parameters\",\"ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1\",\"--num_train_epochs\",\"1\",\"--per_device_eval_batch_size\",\"2\",\"--per_device_train_batch_size\",\"2\",\"--test_dir\",\"/opt/ml/input/data/test\",\"--training_dir\",\"/opt/ml/input/data/train\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=4\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=1e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_MAX_LENGTH=1536\u001b[0m\n",
      "\u001b[34mSM_HP_MP_PARAMETERS={\"ddp\":true,\"fp16\":true,\"partitions\":16,\"pipeline_parallel_degree\":16,\"placement_strategy\":\"cluster\",\"tensor_parallel_degree\":1}\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=2\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_DIR=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINING_DIR=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/lib/python3.8/site-packages/smdistributed/dataparallel/lib:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mmpirun --host algo-1:8,algo-2:8 -np 16 --allow-run-as-root --display-map --tag-output -mca btl_tcp_if_include eth0 -mca oob_tcp_if_include eth0 -mca plm_rsh_no_tree_spawn 1 -bind-to none -map-by slot -mca pml ob1 -mca btl ^openib -mca orte_abort_on_non_zero_status 1 -mca btl_vader_single_copy_mechanism none -x NCCL_MIN_NRINGS=4 -x NCCL_SOCKET_IFNAME=eth0 -x NCCL_DEBUG=INFO -x LD_LIBRARY_PATH -x PATH -x LD_PRELOAD=/opt/conda/lib/python3.8/site-packages/gethostname.cpython-38-x86_64-linux-gnu.so -x SM_HOSTS -x SM_NETWORK_INTERFACE_NAME -x SM_HPS -x SM_USER_ENTRY_POINT -x SM_FRAMEWORK_PARAMS -x SM_RESOURCE_CONFIG -x SM_INPUT_DATA_CONFIG -x SM_OUTPUT_DATA_DIR -x SM_CHANNELS -x SM_CURRENT_HOST -x SM_CURRENT_INSTANCE_TYPE -x SM_CURRENT_INSTANCE_GROUP -x SM_CURRENT_INSTANCE_GROUP_HOSTS -x SM_INSTANCE_GROUPS -x SM_INSTANCE_GROUPS_DICT -x SM_DISTRIBUTION_INSTANCE_GROUPS -x SM_IS_HETERO -x SM_MODULE_NAME -x SM_LOG_LEVEL -x SM_FRAMEWORK_MODULE -x SM_INPUT_DIR -x SM_INPUT_CONFIG_DIR -x SM_OUTPUT_DIR -x SM_NUM_CPUS -x SM_NUM_GPUS -x SM_MODEL_DIR -x SM_MODULE_DIR -x SM_TRAINING_ENV -x SM_USER_ARGS -x SM_OUTPUT_INTERMEDIATE_DIR -x SM_CHANNEL_TEST -x SM_CHANNEL_TRAIN -x SM_HP_GRADIENT_ACCUMULATION_STEPS -x SM_HP_LEARNING_RATE -x SM_HP_MODEL_MAX_LENGTH -x SM_HP_MP_PARAMETERS -x SM_HP_NUM_TRAIN_EPOCHS -x SM_HP_PER_DEVICE_EVAL_BATCH_SIZE -x SM_HP_PER_DEVICE_TRAIN_BATCH_SIZE -x SM_HP_TEST_DIR -x SM_HP_TRAINING_DIR -x PYTHONPATH /opt/conda/bin/python3.8 -m mpi4py train-llama-for-pytorch-container.py --gradient_accumulation_steps 4 --learning_rate 1e-05 --model_max_length 1536 --mp_parameters ddp=True,fp16=True,partitions=16,pipeline_parallel_degree=16,placement_strategy=cluster,tensor_parallel_degree=1 --num_train_epochs 1 --per_device_eval_batch_size 2 --per_device_train_batch_size 2 --test_dir /opt/ml/input/data/test --training_dir /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2023-05-21 03:38:55.030: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34mWarning: Permanently added 'algo-2,10.0.209.56' (ECDSA) to the list of known hosts.\u001b[0m\n",
      "\u001b[34mData for JOB [41012,1] offset 0 Total slots allocated 16\n",
      " ========================   JOB MAP   ========================\n",
      " Data for node: algo-1#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 0 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 1 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 2 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 3 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 4 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 5 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 6 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 7 Bound: N/A\n",
      " Data for node: algo-2#011Num slots: 8#011Max slots: 0#011Num procs: 8\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 8 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 9 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 10 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 11 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 12 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 13 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 14 Bound: N/A\n",
      " #011Process OMPI jobid: [41012,1] App: 0 Process rank: 15 Bound: N/A\n",
      " =============================================================\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,707 sagemaker-training-toolkit INFO     Process[es]: [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,708 sagemaker-training-toolkit INFO     Orted process found [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[35m2023-05-21 03:38:55,708 sagemaker-training-toolkit INFO     Waiting for orted process [psutil.Process(pid=218, name='orted', status='sleeping', started='03:38:55')]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:56.856: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:56.889: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:56.920: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:56.928: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:56.929: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:56.942: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:56.945: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:56.952: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:56.957: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:56.960: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:56.964: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:68] Found unsupported HuggingFace version 4.28.0.dev0 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:2 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:3 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:3 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:4 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:4 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:5 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:5 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:6 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:6 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:7 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:7 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:8 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:8 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:9 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:9 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:10 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:10 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:11 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:11 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:12 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:12 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:13 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:13 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:14 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:14 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:15 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:15 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:16 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:16 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:17 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:17 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:18 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:18 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:19 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:19 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:20 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:20 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:21 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:21 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:22 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:22 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:23 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:23 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:24 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:24 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:25 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:25 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:26 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:26 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:27 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:27 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:28 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:28 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:29 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:29 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:30 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:30 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:31 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:31 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:32 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:32 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:33 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:33 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:34 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:34 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:35 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:35 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:36 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:36 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:37 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:37 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:38 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:38 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:39 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:39 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:40 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:40 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:41 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:41 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:42 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:42 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:43 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:43 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:44 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:44 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:45 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:45 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:46 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:46 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:47 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:47 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:48 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:48 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:49 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:49 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:50 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:50 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:51 to store for rank: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 6: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [6] Finished initializing torch distributed process groups. pp_rank: 6, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [1] Finished initializing torch distributed process groups. pp_rank: 1, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 9: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:58.759: I smdistributed/modelparallel/torch/state_mod.py:162] [9] Finished initializing torch distributed process groups. pp_rank: 9, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 11: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:58.760: I smdistributed/modelparallel/torch/state_mod.py:162] [11] Finished initializing torch distributed process groups. pp_rank: 11, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 15: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:58.761: I smdistributed/modelparallel/torch/state_mod.py:162] [15] Finished initializing torch distributed process groups. pp_rank: 15, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 7: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:58.763: I smdistributed/modelparallel/torch/state_mod.py:162] [7] Finished initializing torch distributed process groups. pp_rank: 7, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:58.763: I smdistributed/modelparallel/torch/state_mod.py:162] [2] Finished initializing torch distributed process groups. pp_rank: 2, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 10: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:58.764: I smdistributed/modelparallel/torch/state_mod.py:162] [10] Finished initializing torch distributed process groups. pp_rank: 10, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 8: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:38:58.764: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:58.764: I smdistributed/modelparallel/torch/state_mod.py:162] [8] Finished initializing torch distributed process groups. pp_rank: 8, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 13: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:58.765: I smdistributed/modelparallel/torch/state_mod.py:162] [13] Finished initializing torch distributed process groups. pp_rank: 13, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:38:58.766: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 12: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 14: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:58.766: I smdistributed/modelparallel/torch/state_mod.py:162] [12] Finished initializing torch distributed process groups. pp_rank: 12, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:58.766: I smdistributed/modelparallel/torch/state_mod.py:162] [14] Finished initializing torch distributed process groups. pp_rank: 14, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:58.767: I smdistributed/modelparallel/torch/state_mod.py:162] [3] Finished initializing torch distributed process groups. pp_rank: 3, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:58.768: I smdistributed/modelparallel/torch/state_mod.py:162] [0] Finished initializing torch distributed process groups. pp_rank: 0, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:58.768: I smdistributed/modelparallel/torch/throttler.py:37] Using NCCL throttle limit of 8.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 5: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:torch.distributed.distributed_c10d:Rank 4: Completed store-based barrier for key:store_based_barrier_key:51 with 16 nodes.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:58.769: I smdistributed/modelparallel/torch/state_mod.py:162] [5] Finished initializing torch distributed process groups. pp_rank: 5, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:58.769: I smdistributed/modelparallel/torch/state_mod.py:162] [4] Finished initializing torch distributed process groups. pp_rank: 4, tp_rank: 0, dp_rank: 0, rdp_rank: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:38:58.772: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:38:58.775: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:38:58.777: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:38:58.778: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:38:58.780: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:38:58.781: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:38:58.782: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:38:58.781: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:293] Configuration parameters:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   pipeline_parallel_degree: 16\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.604: I smdistributed/modelparallel/backend/config.py:296]   pipeline: interleaved\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   horovod: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_reduce_bucket_size: 500000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_param_persistence_threshold: 1000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_max_live_parameters: 1000000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_hierarchical_allgather: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   sdp_gradient_clipping: 1.0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp_port: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   ddp_dist_backend: nccl\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   contiguous: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   placement_strategy: cluster\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   optimize: speed\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   default_partition: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   auto_partition: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.605: I smdistributed/modelparallel/backend/config.py:296]   prescaled_batch: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   memory_weight: 0.8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   active_microbatches: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   fp16: True\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   bf16: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   fp16_params: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   tensor_parallel_seed: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   offload_activations: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   shard_optimizer_state: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   sharded_data_parallel_degree: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   delayed_parameter_initialization: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   skip_tracing: False\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: I smdistributed/modelparallel/backend/config.py:296]   activation_loading_horizon: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.606: W smdistributed/modelparallel/backend/config.py:301] WARNING: \"fp16_params\" is a deprecated config key, please use \"fp16\" instead\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:38:59.607: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:38:59.787: W smdistributed/modelparallel/torch/__init__.py:114] SageMaker model parallelism is initialized already. Ignoring the smp.init() call.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-----------local rank 0 downloading model from s3----\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/.gitattributes /tmp/llama_source/.gitattributes\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/generation_config.json /tmp/llama_source/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/config.json /tmp/llama_source/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/README.md /tmp/llama_source/README.md\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/special_tokens_map.json /tmp/llama_source/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/LICENSE /tmp/llama_source/LICENSE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer_config.json /tmp/llama_source/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model.bin.index.json /tmp/llama_source/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer.model /tmp/llama_source/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded train_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:INFO:__main__: loaded test_dataset length is: 9450\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:-----------local rank 0 downloading model from s3----\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/generation_config.json /tmp/llama_source/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/special_tokens_map.json /tmp/llama_source/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/config.json /tmp/llama_source/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/README.md /tmp/llama_source/README.md\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/.gitattributes /tmp/llama_source/.gitattributes\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer_config.json /tmp/llama_source/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model.bin.index.json /tmp/llama_source/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/LICENSE /tmp/llama_source/LICENSE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/tokenizer.model /tmp/llama_source/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00028-of-00033.bin /tmp/llama_source/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00028-of-00033.bin /tmp/llama_source/pytorch_model-00028-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00024-of-00033.bin /tmp/llama_source/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00031-of-00033.bin /tmp/llama_source/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00024-of-00033.bin /tmp/llama_source/pytorch_model-00024-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00003-of-00033.bin /tmp/llama_source/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00003-of-00033.bin /tmp/llama_source/pytorch_model-00003-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00015-of-00033.bin /tmp/llama_source/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00025-of-00033.bin /tmp/llama_source/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00019-of-00033.bin /tmp/llama_source/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00014-of-00033.bin /tmp/llama_source/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00014-of-00033.bin /tmp/llama_source/pytorch_model-00014-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00031-of-00033.bin /tmp/llama_source/pytorch_model-00031-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00019-of-00033.bin /tmp/llama_source/pytorch_model-00019-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00025-of-00033.bin /tmp/llama_source/pytorch_model-00025-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00015-of-00033.bin /tmp/llama_source/pytorch_model-00015-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00001-of-00033.bin /tmp/llama_source/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00009-of-00033.bin /tmp/llama_source/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00001-of-00033.bin /tmp/llama_source/pytorch_model-00001-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00006-of-00033.bin /tmp/llama_source/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00029-of-00033.bin /tmp/llama_source/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00010-of-00033.bin /tmp/llama_source/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00005-of-00033.bin /tmp/llama_source/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00010-of-00033.bin /tmp/llama_source/pytorch_model-00010-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00005-of-00033.bin /tmp/llama_source/pytorch_model-00005-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00013-of-00033.bin /tmp/llama_source/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00009-of-00033.bin /tmp/llama_source/pytorch_model-00009-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00029-of-00033.bin /tmp/llama_source/pytorch_model-00029-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00018-of-00033.bin /tmp/llama_source/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00026-of-00033.bin /tmp/llama_source/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00026-of-00033.bin /tmp/llama_source/pytorch_model-00026-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00023-of-00033.bin /tmp/llama_source/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00018-of-00033.bin /tmp/llama_source/pytorch_model-00018-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00013-of-00033.bin /tmp/llama_source/pytorch_model-00013-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00002-of-00033.bin /tmp/llama_source/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00022-of-00033.bin /tmp/llama_source/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00023-of-00033.bin /tmp/llama_source/pytorch_model-00023-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00021-of-00033.bin /tmp/llama_source/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00006-of-00033.bin /tmp/llama_source/pytorch_model-00006-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00022-of-00033.bin /tmp/llama_source/pytorch_model-00022-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00021-of-00033.bin /tmp/llama_source/pytorch_model-00021-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00002-of-00033.bin /tmp/llama_source/pytorch_model-00002-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00012-of-00033.bin /tmp/llama_source/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00004-of-00033.bin /tmp/llama_source/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00004-of-00033.bin /tmp/llama_source/pytorch_model-00004-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00032-of-00033.bin /tmp/llama_source/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00020-of-00033.bin /tmp/llama_source/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00032-of-00033.bin /tmp/llama_source/pytorch_model-00032-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00012-of-00033.bin /tmp/llama_source/pytorch_model-00012-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00020-of-00033.bin /tmp/llama_source/pytorch_model-00020-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00017-of-00033.bin /tmp/llama_source/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00033-of-00033.bin /tmp/llama_source/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00017-of-00033.bin /tmp/llama_source/pytorch_model-00017-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00008-of-00033.bin /tmp/llama_source/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00008-of-00033.bin /tmp/llama_source/pytorch_model-00008-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00033-of-00033.bin /tmp/llama_source/pytorch_model-00033-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00027-of-00033.bin /tmp/llama_source/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00027-of-00033.bin /tmp/llama_source/pytorch_model-00027-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00016-of-00033.bin /tmp/llama_source/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00011-of-00033.bin /tmp/llama_source/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00016-of-00033.bin /tmp/llama_source/pytorch_model-00016-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00011-of-00033.bin /tmp/llama_source/pytorch_model-00011-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00030-of-00033.bin /tmp/llama_source/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00030-of-00033.bin /tmp/llama_source/pytorch_model-00030-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00007-of-00033.bin /tmp/llama_source/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:cp s3://sagemaker-us-east-1-514385905925/llama/pretrained/7B/model/pytorch_model-00007-of-00033.bin /tmp/llama_source/pytorch_model-00007-of-00033.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.25it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:15,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:13,  2.24it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:16,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:14,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:14,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.23it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:13,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:15,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:02<00:14,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:15,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:12,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:13,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:15,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:14,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:16,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   3%|â         | 1/33 [00:00<00:15,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:15,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:14,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:11,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:01<00:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   6%|â         | 2/33 [00:00<00:15,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:12,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:03<00:13,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:11,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:   9%|â         | 3/33 [00:01<00:15,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:14,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.19it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:01<00:14,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  12%|ââ        | 4/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:11,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.04it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  15%|ââ        | 5/33 [00:02<00:14,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:02<00:13,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  18%|ââ        | 6/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  21%|ââ        | 7/33 [00:03<00:13,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:05<00:09,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:03<00:12,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  24%|âââ       | 8/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:05<00:09,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:10,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.02it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  27%|âââ       | 9/33 [00:04<00:12,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:06<00:08,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:07<00:09,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.01it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:11,  1.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:04<00:11,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:06<00:08,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  30%|âââ       | 10/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:09,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:07,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.07it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.01it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:07<00:07,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:06<00:11,  1.89it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  33%|ââââ      | 11/33 [00:05<00:11,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:07<00:07,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:08<00:08,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:05<00:10,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  36%|ââââ      | 12/33 [00:06<00:10,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:10,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:06,  2.21it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:08<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:09,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  39%|ââââ      | 13/33 [00:06<00:10,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:07<00:10,  1.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:08<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:08<00:06,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:08<00:06,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:06<00:09,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  42%|âââââ     | 14/33 [00:07<00:09,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:09,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:05,  2.22it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:09<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:09<00:05,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  45%|âââââ     | 15/33 [00:07<00:08,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:09<00:05,  2.20it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:09<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:08<00:09,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:09<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.03it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:07<00:08,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  48%|âââââ     | 16/33 [00:08<00:08,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.18it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.03it/s][1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  52%|ââââââ    | 17/33 [00:08<00:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:09<00:08,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:08<00:07,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  55%|ââââââ    | 18/33 [00:09<00:07,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:10<00:04,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.17it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:07,  1.84it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:10<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  58%|ââââââ    | 19/33 [00:09<00:06,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:11<00:03,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:10<00:07,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:09<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  61%|ââââââ    | 20/33 [00:10<00:06,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:11<00:03,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.16it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:11<00:03,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:11<00:06,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.13it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  64%|âââââââ   | 21/33 [00:10<00:05,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:12<00:02,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:10<00:05,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:12<00:02,  2.12it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:11<00:06,  1.81it/s][1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  67%|âââââââ   | 22/33 [00:11<00:05,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:03,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:12<00:02,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.05it/s][1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:11<00:05,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:13<00:01,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  70%|âââââââ   | 23/33 [00:12<00:05,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:11<00:04,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:13<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  73%|ââââââââ  | 24/33 [00:12<00:04,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.11it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:13<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:02,  1.98it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:14<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:12<00:04,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  76%|ââââââââ  | 25/33 [00:13<00:04,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.12it/s][1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:12<00:03,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:14<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:13<00:03,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  1.99it/s][1,mpirank:12,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  2.15it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  79%|ââââââââ  | 26/33 [00:13<00:03,  1.79it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  1.97it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:14<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:01,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  1.96it/s]#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  2.14it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  1.96it/s][1,mpirank:2,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:13<00:03,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  1.95it/s][1,mpirank:8,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:15<00:00,  2.09it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  82%|âââââââââ | 27/33 [00:14<00:03,  1.80it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:13<00:02,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.10it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  1.96it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.91it/s][1,mpirank:7,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.91it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:14<00:02,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.93it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  85%|âââââââââ | 28/33 [00:14<00:02,  1.81it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.90it/s]#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.06it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.85it/s][1,mpirank:13,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:14<00:01,  2.00it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:14<00:01,  2.04it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  88%|âââââââââ | 29/33 [00:15<00:02,  1.82it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:15<00:01,  2.01it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.08it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  91%|âââââââââ | 30/33 [00:15<00:01,  1.83it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:15<00:00,  2.02it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.07it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.95it/s][1,mpirank:11,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  94%|ââââââââââ| 31/33 [00:16<00:01,  1.85it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:15<00:00,  2.05it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.95it/s][1,mpirank:10,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  2.03it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards:  97%|ââââââââââ| 32/33 [00:17<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.95it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:16<00:00,  1.99it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:17<00:00,  1.74it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:#015Loading checkpoint shards: 100%|ââââââââââ| 33/33 [00:17<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The tokenizer class you load from this checkpoint is 'LLaMATokenizer'. \u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:The class this function is called from is 'LlamaTokenizer'.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:-----------no pad token and add special token PAD----\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:Using pad_token, but it is not set yet.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:We have added 3 tokens\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.581: I smdistributed/modelparallel/torch/model.py:137] [12] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.603: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [12] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.653 algo-2:225 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:20.779 algo-2:225 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.789: I smdistributed/modelparallel/torch/model.py:137] [6] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.810: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [6] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.859 algo-1:221 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:20.981 algo-1:221 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.284: I smdistributed/modelparallel/torch/model.py:137] [8] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.305: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [8] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.351: I smdistributed/modelparallel/torch/model.py:137] [3] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.354 algo-2:221 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.374: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [3] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.429 algo-1:218 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:21.479 algo-2:221 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:21.564 algo-1:218 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.575: I smdistributed/modelparallel/torch/model.py:137] [2] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.597: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [2] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.649 algo-1:217 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:21.779 algo-1:217 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.060: I smdistributed/modelparallel/torch/model.py:137] [13] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.081: I smdistributed/modelparallel/torch/model.py:137] [4] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.081: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [13] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.087: I smdistributed/modelparallel/torch/model.py:137] [7] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.103: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [4] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.108: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [7] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.131 algo-2:226 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.152 algo-1:219 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.157 algo-1:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:22.255 algo-2:226 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:22.277 algo-1:219 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:22.281 algo-1:222 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.521: I smdistributed/modelparallel/torch/model.py:137] [0] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.542: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [0] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 0/200 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.591 algo-1:215 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.714 algo-1:215 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.722: W smdistributed/modelparallel/backend/split.py:167] Non-splittable object of type <class 'transformers.tokenization_utils_base.BatchEncoding'> passed to smp.step. If this object contains tensors that need to be split across microbatches, implement a 'smp_slice' method for this class. See SMP documentation for further information.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:22.723: I smdistributed/modelparallel/torch/worker.py:300] Tracing on GPU. If the model parameters do not fit in a single GPU, you can set trace_device to `cpu`.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:22.949: I smdistributed/modelparallel/torch/model.py:137] [5] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:22.973: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [5] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:23.025 algo-1:220 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:23.158 algo-1:220 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.203: I smdistributed/modelparallel/torch/model.py:137] [1] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.225: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [1] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.248: I smdistributed/modelparallel/torch/model.py:137] [14] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.271: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [14] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.276 algo-1:216 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.325 algo-2:227 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:23.402 algo-1:216 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:23.460 algo-2:227 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.005: I smdistributed/modelparallel/torch/model.py:137] [11] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.027: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [11] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.076 algo-2:224 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:24.201 algo-2:224 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.457: I smdistributed/modelparallel/torch/model.py:137] [10] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.478: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [10] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.526 algo-2:223 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:24.648 algo-2:223 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.692: I smdistributed/modelparallel/torch/model.py:137] [15] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.712: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [15] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.760 algo-2:228 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:24.882 algo-2:228 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.776: I smdistributed/modelparallel/torch/model.py:137] [9] FP16_Module initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.797: I smdistributed/modelparallel/torch/optimizers/optimizer.py:503] [9] FP16_Optimizer initialized\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.846 algo-2:222 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.19b20220829-py3.8.egg/smdebug/profiler/system_metrics_reader.py:63: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:25.967 algo-2:222 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:670] Partition assignments:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/lm_head: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/embed_tokens: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/norm: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.011: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/0: 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/1: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/2: 2\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/3: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/4: 3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/5: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/6: 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/7: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/8: 5\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/9: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/10: 6\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/11: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/12: 7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/13: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/14: 8\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/15: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.012: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/16: 9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/17: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/18: 10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/19: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/20: 11\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/21: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/22: 12\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/23: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/24: 13\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/25: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/26: 14\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/27: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/28: 15\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/29: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/30: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:30.013: I smdistributed/modelparallel/torch/model.py:679] main/module/module/module/model/layers/31: 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:30.172: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 6 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:40:30.174: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 6 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:30.183: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 9 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:40:30.185: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 9 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:30.190: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 13 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:30.190: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 3 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:40:30.191: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 13 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:40:30.192: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 3 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:30.192: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 11 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:40:30.193: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 11 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:30.199: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 15 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:40:30.200: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 15 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:30.242: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 2 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:40:30.243: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 2 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:30.244: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 8 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:40:30.246: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 8 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:30.251: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 12 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:30.252: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 10 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 12 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 10 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:30.253: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 14 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:40:30.255: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 14 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:30.255: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 5 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:40:30.256: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 5 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:30.256: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 1 are 27. 27 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:40:30.258: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 1 are 3.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:30.260: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 7 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:40:30.261: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 7 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:30.261: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 4 are 18. 18 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:40:30.262: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 4 are 2.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:38.778: I smdistributed/modelparallel/torch/model.py:604] Number of parameters on partition 0 are 12. 12 require grads\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:38.779: I smdistributed/modelparallel/torch/model.py:633] Number of buffers on partition 0 are 1.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:40:39.491: I smdistributed/modelparallel/torch/model.py:730] Finished partitioning the model\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1365: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stderr>:  warnings.warn(\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO Bootstrap : Using eth0:10.0.209.56<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO Bootstrap : Using eth0:10.0.248.37<0>\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v4 symbol.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Using aws-ofi-nccl 1.3.0aws\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /usr/local/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:228 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:223 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:221 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:219 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:220 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:222 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:226 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:216 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:227 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:218 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:221 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:217 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:224 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Forcing AWS OFI ndev 4\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:222 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO NET/OFI Selected Provider is efa\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:225 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:NCCL version 2.10.3+cuda11.3\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2026 [0] NCCL INFO comm 0x7f0c840030d0 rank 0 nranks 1 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2028 [7] NCCL INFO comm 0x7f33100030d0 rank 0 nranks 1 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO NCCL_MIN_NRINGS set by environment to 4.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 00/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 01/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2032 [5] NCCL INFO comm 0x7f94d00030d0 rank 0 nranks 1 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 02/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 03/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 04/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 05/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 06/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 07/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 08/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 09/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 10/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 11/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 12/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 13/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 14/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 15/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 16/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 17/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2039 [7] NCCL INFO comm 0x7f7a040030d0 rank 0 nranks 1 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 18/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 19/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2032 [0] NCCL INFO comm 0x7f72d00030d0 rank 0 nranks 1 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 20/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 21/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 22/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 23/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 24/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 25/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 26/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 27/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 28/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 29/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 30/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Channel 31/32 :    0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Trees [0] -1/-1/-1->0->-1 [1] -1/-1/-1->0->-1 [2] -1/-1/-1->0->-1 [3] -1/-1/-1->0->-1 [4] -1/-1/-1->0->-1 [5] -1/-1/-1->0->-1 [6] -1/-1/-1->0->-1 [7] -1/-1/-1->0->-1 [8] -1/-1/-1->0->-1 [9] -1/-1/-1->0->-1 [10] -1/-1/-1->0->-1 [11] -1/-1/-1->0->-1 [12] -1/-1/-1->0->-1 [13] -1/-1/-1->0->-1 [14] -1/-1/-1->0->-1 [15] -1/-1/-1->0->-1 [16] -1/-1/-1->0->-1 [17] -1/-1/-1->0->-1 [18] -1/-1/-1->0->-1 [19] -1/-1/-1->0->-1 [20] -1/-1/-1->0->-1 [21] -1/-1/-1->0->-1 [22] -1/-1/-1->0->-1 [23] -1/-1/-1->0->-1 [24] -1/-1/-1->0->-1 [25] -1/-1/-1->0->-1 [26] -1/-1/-1->0->-1 [27] -1/-1/-1->0->-1 [28] -1/-1/-1->0->-1 [29] -1/-1/-1->0->-1 [30] -1/-1/-1->0->-1 [31] -1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2035 [5] NCCL INFO comm 0x7f102c0030d0 rank 0 nranks 1 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2033 [4] NCCL INFO comm 0x7f55d00030d0 rank 0 nranks 1 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2040 [6] NCCL INFO comm 0x7f9d700030d0 rank 0 nranks 1 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2026 [2] NCCL INFO comm 0x7fcd600030d0 rank 0 nranks 1 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2044 [1] NCCL INFO comm 0x7f8a900030d0 rank 0 nranks 1 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2042 [3] NCCL INFO comm 0x7efee00030d0 rank 0 nranks 1 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2045 [4] NCCL INFO comm 0x7f20100030d0 rank 0 nranks 1 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2043 [2] NCCL INFO comm 0x7fa3d00030d0 rank 0 nranks 1 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2042 [6] NCCL INFO comm 0x7ef8980030d0 rank 0 nranks 1 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO 32 coll channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2044 [1] NCCL INFO comm 0x7ff2740030d0 rank 0 nranks 1 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2041 [3] NCCL INFO comm 0x7fce280030d0 rank 0 nranks 1 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [2] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [0] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [3] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [4] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [6] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [5] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [7] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [14] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [10] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [15] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 03:41:14.128: I smdistributed/modelparallel/torch/ddp_model.py:632] [11] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [9] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [12] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [8] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [13] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 03:41:14.129: I smdistributed/modelparallel/torch/ddp_model.py:632] [1] Reducer buckets have been rebuilt in this iteration.\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/0/-1->8->-1 [3] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/2/-1->10->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->8 [3] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 02 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2133 [6] NCCL INFO comm 0x7f9c180030d0 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2130 [5] NCCL INFO comm 0x7f0ebc0030d0 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2135 [3] NCCL INFO comm 0x7efd640030d0 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2129 [7] NCCL INFO comm 0x7f30b80030d0 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2134 [4] NCCL INFO comm 0x7f1dbc0030d0 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2128 [2] NCCL INFO comm 0x7fcbe40030d0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2131 [1] NCCL INFO comm 0x7f89040030d0 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2132 [0] NCCL INFO comm 0x7f714c0030d0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2131 [2] NCCL INFO comm 0x7fa06c0030d0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2130 [0] NCCL INFO comm 0x7f072c0030d0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2132 [7] NCCL INFO comm 0x7f78280030d0 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2133 [3] NCCL INFO comm 0x7fcc740030d0 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2135 [6] NCCL INFO comm 0x7ef50c0030d0 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2137 [1] NCCL INFO comm 0x7fedf00030d0 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2136 [5] NCCL INFO comm 0x7f92fc0030d0 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2134 [4] NCCL INFO comm 0x7f54100030d0 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  0%|          | 1/200 [00:55<3:03:03, 55.19s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  0%|          | 1/200 [00:53<2:58:58, 53.96s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  1%|          | 2/200 [01:14<1:52:20, 34.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  1%|          | 2/200 [01:13<1:50:40, 33.54s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|â         | 3/200 [01:33<1:28:54, 27.08s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|â         | 3/200 [01:31<1:28:00, 26.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|â         | 4/200 [01:52<1:17:47, 23.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|â         | 4/200 [01:50<1:17:14, 23.65s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  2%|â         | 5/200 [02:10<1:11:32, 22.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  2%|â         | 5/200 [02:09<1:11:11, 21.91s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  3%|â         | 6/200 [02:29<1:07:37, 20.91s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  3%|â         | 6/200 [02:28<1:07:23, 20.84s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|â         | 7/200 [02:48<1:05:02, 20.22s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|â         | 7/200 [02:47<1:04:53, 20.17s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|â         | 8/200 [03:07<1:03:14, 19.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|â         | 8/200 [03:05<1:03:08, 19.73s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  4%|â         | 9/200 [03:25<1:01:56, 19.46s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  4%|â         | 9/200 [03:24<1:01:52, 19.44s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  5%|â         | 10/200 [03:44<1:00:58, 19.25s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  5%|â         | 10/200 [03:43<1:00:55, 19.24s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|â         | 11/200 [04:03<1:00:11, 19.11s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|â         | 11/200 [04:02<1:00:09, 19.10s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|â         | 12/200 [04:22<59:33, 19.01s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|â         | 12/200 [04:21<59:31, 19.00s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  6%|â         | 13/200 [04:41<59:01, 18.94s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  6%|â         | 13/200 [04:39<59:00, 18.93s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  7%|â         | 14/200 [04:59<58:34, 18.90s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  7%|â         | 14/200 [04:58<58:33, 18.89s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|â         | 15/200 [05:18<58:09, 18.86s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|â         | 15/200 [05:17<58:09, 18.86s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|â         | 16/200 [05:37<57:45, 18.84s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|â         | 16/200 [05:36<57:45, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  8%|â         | 17/200 [05:56<57:25, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  8%|â         | 17/200 [05:55<57:24, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015  9%|â         | 18/200 [06:15<57:04, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015  9%|â         | 18/200 [06:13<57:04, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|â         | 19/200 [06:33<56:45, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|â         | 19/200 [06:32<56:45, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|â         | 20/200 [06:52<56:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|â         | 20/200 [06:51<56:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 10%|â         | 21/200 [07:11<56:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 10%|â         | 21/200 [07:10<56:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 11%|â         | 22/200 [07:30<55:46, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 11%|â         | 22/200 [07:29<55:46, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|ââ        | 23/200 [07:49<55:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|ââ        | 23/200 [07:47<55:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|ââ        | 24/200 [08:07<55:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|ââ        | 24/200 [08:06<55:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 12%|ââ        | 25/200 [08:26<54:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 12%|ââ        | 25/200 [08:25<54:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 13%|ââ        | 26/200 [08:45<54:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 13%|ââ        | 26/200 [08:44<54:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|ââ        | 27/200 [09:04<54:11, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|ââ        | 27/200 [09:02<54:11, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|ââ        | 28/200 [09:23<53:53, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|ââ        | 28/200 [09:21<53:53, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 14%|ââ        | 29/200 [09:41<53:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 14%|ââ        | 29/200 [09:40<53:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 15%|ââ        | 30/200 [10:00<53:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 15%|ââ        | 30/200 [09:59<53:15, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|ââ        | 31/200 [10:19<52:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|ââ        | 31/200 [10:18<52:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|ââ        | 32/200 [10:38<52:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|ââ        | 32/200 [10:36<52:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 16%|ââ        | 33/200 [10:57<52:19, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 16%|ââ        | 33/200 [10:55<52:19, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 17%|ââ        | 34/200 [11:15<52:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 17%|ââ        | 34/200 [11:14<52:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|ââ        | 35/200 [11:34<51:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|ââ        | 35/200 [11:33<51:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|ââ        | 36/200 [11:53<51:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|ââ        | 36/200 [11:52<51:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 18%|ââ        | 37/200 [12:12<51:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 18%|ââ        | 37/200 [12:10<51:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 19%|ââ        | 38/200 [12:30<50:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 19%|ââ        | 38/200 [12:29<50:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|ââ        | 39/200 [12:49<50:24, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|ââ        | 39/200 [12:48<50:24, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|ââ        | 40/200 [13:08<50:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|ââ        | 40/200 [13:07<50:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 20%|ââ        | 41/200 [13:27<49:48, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 20%|ââ        | 41/200 [13:26<49:48, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 21%|ââ        | 42/200 [13:46<49:29, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 21%|ââ        | 42/200 [13:44<49:29, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|âââ       | 43/200 [14:04<49:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|âââ       | 43/200 [14:03<49:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|âââ       | 44/200 [14:23<48:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|âââ       | 44/200 [14:22<48:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 22%|âââ       | 45/200 [14:42<48:33, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 22%|âââ       | 45/200 [14:41<48:33, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 23%|âââ       | 46/200 [15:01<48:13, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 23%|âââ       | 46/200 [15:00<48:13, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|âââ       | 47/200 [15:20<47:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|âââ       | 47/200 [15:18<47:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|âââ       | 48/200 [15:38<47:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|âââ       | 48/200 [15:37<47:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 24%|âââ       | 49/200 [15:57<47:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 24%|âââ       | 49/200 [15:56<47:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|âââ       | 50/200 [16:16<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 25%|âââ       | 50/200 [16:15<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO NET/OFI [1] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO NET/OFI [3] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO NET/OFI [7] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO NET/OFI [5] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO NET/OFI [6] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20/\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO NET/OFI [4] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO NET/OFI [2] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 0 busId 0000:10:1c.0 path /sys/devices/pci0000:10/\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 1 busId 0000:10:1d.0 path /sys/devices/pci0000:10\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 2 busId 0000:20:1c.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO NET/OFI [0] getCudaPath dev 3 busId 0000:20:1d.0 path /sys/devices/pci0000:20\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] -1/-1/-1->1->0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02/04 :    0   7   6   5   4   3   2   1   8  15  14  13  12  11  10   9\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03/04 :    0   3  10  15  14  13  12   9   8  11   2   7   6   5   4   1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Trees [0] 1/8/-1->0->-1 [1] 1/-1/-1->0->7 [2] 1/-1/-1->0->8 [3] 1/-1/-1->0->7\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/10/-1->2->-1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] 0/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] 0/-1/-1->7->6\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Trees [0] 9/-1/-1->8->0 [1] 9/-1/-1->8->15 [2] 9/0/-1->8->-1 [3] 9/-1/-1->8->15\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Trees [0] 13/-1/-1->12->11 [1] 13/-1/-1->12->11 [2] 13/-1/-1->12->11 [3] 13/-1/-1->12->11\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Trees [0] -1/-1/-1->15->14 [1] 8/-1/-1->15->14 [2] -1/-1/-1->15->14 [3] 8/-1/-1->15->14\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Trees [0] 14/-1/-1->13->12 [1] 14/-1/-1->13->12 [2] 14/-1/-1->13->12 [3] 14/-1/-1->13->12\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Trees [0] 10/-1/-1->9->8 [1] -1/-1/-1->9->8 [2] 10/-1/-1->9->8 [3] -1/-1/-1->9->8\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Trees [0] 11/-1/-1->10->9 [1] 11/-1/-1->10->2 [2] 11/-1/-1->10->9 [3] 11/2/-1->10->-1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Trees [0] 15/-1/-1->14->13 [1] 15/-1/-1->14->13 [2] 15/-1/-1->14->13 [3] 15/-1/-1->14->13\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Trees [0] 12/-1/-1->11->10 [1] 12/-1/-1->11->10 [2] 12/-1/-1->11->10 [3] 12/-1/-1->11->10\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 00 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 01 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 00 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 00 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 00 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 02 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 01 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 01 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 02 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 02 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 03 : 13[901d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 03 : 6[a01c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 01 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 03 : 14[a01c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 02 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 03 : 5[901d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 9[101d0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 1[101d0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 00 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 01 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 3[201d0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 02 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 03 : 7[a01d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 00 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 00 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 02 : 2[201c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 01 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 00 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 02 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 00 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 02 : 10[201c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 03 : 15[a01d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 00 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 02 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 02 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 01 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 00 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 01 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 03 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 02 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 03 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 00 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 02 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 00 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 01 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 1[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 00 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 02 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 9[101d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 00 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Channel 03 : 6[a01c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 02 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 00 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 02 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 00 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 01 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 00 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 02 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 00 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 02 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Channel 03 : 14[a01c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 00 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 00 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 12[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 02 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 00 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 02 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 01 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 01 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 4[901c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 01 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 02 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 03 : 0[101c0] -> 7[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 01 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 13[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 00 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 02 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 5[901d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 03 : 8[101c0] -> 15[a01d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 02 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 00 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Channel 03 : 13[901d0] -> 14[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Channel 02 : 1[101d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Channel 03 : 5[901d0] -> 6[a01c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 01 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Channel 02 : 9[101d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Channel 03 : 7[a01d0] -> 0[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 01 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Channel 03 : 15[a01d0] -> 8[101c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 01 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 01 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 01 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Channel 03 : 4[901c0] -> 3[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Channel 03 : 3[201d0] -> 2[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 01 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Channel 03 : 12[901c0] -> 11[201d0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Channel 03 : 11[201d0] -> 10[201c0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [receive] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [receive] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 00 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 01 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 01 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 00 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Channel 02 : 0[101c0] -> 8[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Channel 03 : 10[201c0] -> 2[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Channel 03 : 2[201c0] -> 10[201c0] [send] via NET/AWS Libfabric/1\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Channel 02 : 8[101c0] -> 0[101c0] [send] via NET/AWS Libfabric/0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:algo-2:227:2158 [6] NCCL INFO comm 0x7f9c18007a90 rank 14 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:algo-2:224:2154 [3] NCCL INFO comm 0x7efd64007a90 rank 11 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:algo-2:223:2157 [2] NCCL INFO comm 0x7fcbe4007cc0 rank 10 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:algo-2:228:2159 [7] NCCL INFO comm 0x7f30b8007a90 rank 15 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:algo-2:226:2152 [5] NCCL INFO comm 0x7f0ebc007a90 rank 13 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:algo-2:222:2153 [1] NCCL INFO comm 0x7f8904007a90 rank 9 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:algo-2:221:2156 [0] NCCL INFO comm 0x7f714c007cc0 rank 8 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:algo-2:225:2155 [4] NCCL INFO comm 0x7f1dbc007a90 rank 12 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO threadThresholds 8/8/64 | 128/8/64 | 8/8/512\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO 4 coll channels, 4 p2p channels, 1 p2p channels per peer\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:algo-1:217:2160 [2] NCCL INFO comm 0x7fa06c007cc0 rank 2 nranks 16 cudaDev 2 busId 201c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:algo-1:220:2159 [5] NCCL INFO comm 0x7f92fc007a90 rank 5 nranks 16 cudaDev 5 busId 901d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:algo-1:216:2162 [1] NCCL INFO comm 0x7fedf0007a90 rank 1 nranks 16 cudaDev 1 busId 101d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:algo-1:222:2156 [7] NCCL INFO comm 0x7f7828007a90 rank 7 nranks 16 cudaDev 7 busId a01d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:algo-1:221:2161 [6] NCCL INFO comm 0x7ef50c007a90 rank 6 nranks 16 cudaDev 6 busId a01c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:algo-1:218:2158 [3] NCCL INFO comm 0x7fcc74007a90 rank 3 nranks 16 cudaDev 3 busId 201d0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:2155 [0] NCCL INFO comm 0x7f06680030d0 rank 0 nranks 16 cudaDev 0 busId 101c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:algo-1:219:2157 [4] NCCL INFO comm 0x7f5410007a90 rank 4 nranks 16 cudaDev 4 busId 901c0 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:algo-1:215:215 [0] NCCL INFO Launch mode Parallel\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 4.3493, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                #015[1,mpirank:0,algo-1]<stderr>:#015 25%|âââ       | 50/200 [16:17<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 4.3493, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 25%|âââ       | 50/200 [16:18<46:59, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|âââ       | 51/200 [16:37<48:24, 19.50s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|âââ       | 51/200 [16:36<48:24, 19.50s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|âââ       | 52/200 [16:56<47:34, 19.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|âââ       | 52/200 [16:55<47:34, 19.29s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 26%|âââ       | 53/200 [17:15<46:54, 19.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 26%|âââ       | 53/200 [17:14<46:54, 19.14s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 27%|âââ       | 54/200 [17:34<46:19, 19.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 27%|âââ       | 54/200 [17:32<46:19, 19.04s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|âââ       | 55/200 [17:52<45:50, 18.97s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|âââ       | 55/200 [17:51<45:50, 18.97s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|âââ       | 56/200 [18:11<45:24, 18.92s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|âââ       | 56/200 [18:10<45:24, 18.92s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 28%|âââ       | 57/200 [18:30<44:59, 18.88s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 28%|âââ       | 57/200 [18:29<44:59, 18.88s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 29%|âââ       | 58/200 [18:49<44:36, 18.85s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 29%|âââ       | 58/200 [18:47<44:36, 18.85s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|âââ       | 59/200 [19:07<44:15, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|âââ       | 59/200 [19:06<44:15, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|âââ       | 60/200 [19:26<43:54, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|âââ       | 60/200 [19:25<43:54, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 30%|âââ       | 61/200 [19:45<43:34, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 30%|âââ       | 61/200 [19:44<43:34, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 31%|âââ       | 62/200 [20:04<43:14, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 31%|âââ       | 62/200 [20:03<43:14, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|ââââ      | 63/200 [20:23<42:56, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|ââââ      | 63/200 [20:21<42:56, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|ââââ      | 64/200 [20:41<42:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|ââââ      | 64/200 [20:40<42:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 32%|ââââ      | 65/200 [21:00<42:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 32%|ââââ      | 65/200 [20:59<42:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 33%|ââââ      | 66/200 [21:19<41:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 33%|ââââ      | 66/200 [21:18<41:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|ââââ      | 67/200 [21:38<41:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|ââââ      | 67/200 [21:37<41:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|ââââ      | 68/200 [21:57<41:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|ââââ      | 68/200 [21:55<41:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 34%|ââââ      | 69/200 [22:15<41:01, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 34%|ââââ      | 69/200 [22:14<41:01, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 35%|ââââ      | 70/200 [22:34<40:42, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 35%|ââââ      | 70/200 [22:33<40:42, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|ââââ      | 71/200 [22:53<40:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|ââââ      | 71/200 [22:52<40:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|ââââ      | 72/200 [23:12<40:05, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|ââââ      | 72/200 [23:11<40:05, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 36%|ââââ      | 73/200 [23:31<39:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 36%|ââââ      | 73/200 [23:29<39:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 37%|ââââ      | 74/200 [23:49<39:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 37%|ââââ      | 74/200 [23:48<39:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|ââââ      | 75/200 [24:08<39:09, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|ââââ      | 75/200 [24:07<39:09, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|ââââ      | 76/200 [24:27<38:50, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|ââââ      | 76/200 [24:26<38:50, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 38%|ââââ      | 77/200 [24:46<38:31, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 38%|ââââ      | 77/200 [24:44<38:31, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 39%|ââââ      | 78/200 [25:05<38:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 39%|ââââ      | 78/200 [25:03<38:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|ââââ      | 79/200 [25:23<37:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|ââââ      | 79/200 [25:22<37:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|ââââ      | 80/200 [25:42<37:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|ââââ      | 80/200 [25:41<37:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 40%|ââââ      | 81/200 [26:01<37:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 40%|ââââ      | 81/200 [26:00<37:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 41%|ââââ      | 82/200 [26:20<36:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 41%|ââââ      | 82/200 [26:19<36:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|âââââ     | 83/200 [26:39<36:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|âââââ     | 83/200 [26:37<36:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|âââââ     | 84/200 [26:57<36:20, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|âââââ     | 84/200 [26:56<36:20, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 42%|âââââ     | 85/200 [27:16<36:01, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 42%|âââââ     | 85/200 [27:15<36:01, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 43%|âââââ     | 86/200 [27:35<35:41, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 43%|âââââ     | 86/200 [27:34<35:41, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|âââââ     | 87/200 [27:54<35:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|âââââ     | 87/200 [27:52<35:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|âââââ     | 88/200 [28:12<35:03, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|âââââ     | 88/200 [28:11<35:03, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 44%|âââââ     | 89/200 [28:31<34:44, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 44%|âââââ     | 89/200 [28:30<34:44, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 45%|âââââ     | 90/200 [28:50<34:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 45%|âââââ     | 90/200 [28:49<34:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|âââââ     | 91/200 [29:09<34:06, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|âââââ     | 91/200 [29:08<34:06, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|âââââ     | 92/200 [29:28<33:48, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|âââââ     | 92/200 [29:26<33:48, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 46%|âââââ     | 93/200 [29:46<33:29, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 46%|âââââ     | 93/200 [29:45<33:29, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 47%|âââââ     | 94/200 [30:05<33:10, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 47%|âââââ     | 94/200 [30:04<33:10, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|âââââ     | 95/200 [30:24<32:51, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|âââââ     | 95/200 [30:23<32:51, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|âââââ     | 96/200 [30:43<32:32, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|âââââ     | 96/200 [30:41<32:32, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 48%|âââââ     | 97/200 [31:01<32:13, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 48%|âââââ     | 97/200 [31:00<32:13, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 49%|âââââ     | 98/200 [31:20<31:55, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 49%|âââââ     | 98/200 [31:19<31:55, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|âââââ     | 99/200 [31:39<31:36, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|âââââ     | 99/200 [31:38<31:36, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|âââââ     | 100/200 [31:58<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|âââââ     | 100/200 [31:57<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.1956, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|âââââ     | 100/200 [31:57<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.1956, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                 #015[1,mpirank:8,algo-2]<stderr>:#015 50%|âââââ     | 100/200 [31:58<31:17, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 50%|âââââ     | 101/200 [32:17<30:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 50%|âââââ     | 101/200 [32:15<30:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 51%|âââââ     | 102/200 [32:35<30:40, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 51%|âââââ     | 102/200 [32:34<30:40, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|ââââââ    | 103/200 [32:54<30:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|ââââââ    | 103/200 [32:53<30:22, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|ââââââ    | 104/200 [33:13<30:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|ââââââ    | 104/200 [33:12<30:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 52%|ââââââ    | 105/200 [33:32<29:45, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 52%|ââââââ    | 105/200 [33:30<29:45, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 53%|ââââââ    | 106/200 [33:51<29:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 53%|ââââââ    | 106/200 [33:49<29:26, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|ââââââ    | 107/200 [34:09<29:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|ââââââ    | 107/200 [34:08<29:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 54%|ââââââ    | 108/200 [34:28<28:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 54%|ââââââ    | 108/200 [34:27<28:48, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|ââââââ    | 109/200 [34:47<28:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|ââââââ    | 109/200 [34:46<28:29, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 55%|ââââââ    | 110/200 [35:06<28:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 55%|ââââââ    | 110/200 [35:04<28:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|ââââââ    | 111/200 [35:24<27:52, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|ââââââ    | 111/200 [35:23<27:52, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|ââââââ    | 112/200 [35:43<27:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|ââââââ    | 112/200 [35:42<27:34, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 56%|ââââââ    | 113/200 [36:02<27:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 56%|ââââââ    | 113/200 [36:01<27:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|ââââââ    | 114/200 [36:21<26:56, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|ââââââ    | 114/200 [36:20<26:56, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 57%|ââââââ    | 115/200 [36:40<26:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 57%|ââââââ    | 115/200 [36:38<26:37, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|ââââââ    | 116/200 [36:58<26:19, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|ââââââ    | 116/200 [36:57<26:19, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 58%|ââââââ    | 117/200 [37:17<26:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 58%|ââââââ    | 117/200 [37:16<26:00, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 59%|ââââââ    | 118/200 [37:36<25:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 59%|ââââââ    | 118/200 [37:35<25:41, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|ââââââ    | 119/200 [37:55<25:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|ââââââ    | 119/200 [37:54<25:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|ââââââ    | 120/200 [38:14<25:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|ââââââ    | 120/200 [38:12<25:03, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 60%|ââââââ    | 121/200 [38:32<24:44, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 60%|ââââââ    | 121/200 [38:31<24:44, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 61%|ââââââ    | 122/200 [38:51<24:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 61%|ââââââ    | 122/200 [38:50<24:25, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|âââââââ   | 123/200 [39:10<24:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|âââââââ   | 123/200 [39:09<24:06, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|âââââââ   | 124/200 [39:29<23:47, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|âââââââ   | 124/200 [39:28<23:47, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 62%|âââââââ   | 125/200 [39:48<23:28, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 62%|âââââââ   | 125/200 [39:46<23:28, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 63%|âââââââ   | 126/200 [40:06<23:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 63%|âââââââ   | 126/200 [40:05<23:10, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|âââââââ   | 127/200 [40:25<22:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|âââââââ   | 127/200 [40:24<22:51, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|âââââââ   | 128/200 [40:44<22:32, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|âââââââ   | 128/200 [40:43<22:32, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 64%|âââââââ   | 129/200 [41:03<22:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 64%|âââââââ   | 129/200 [41:01<22:14, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 65%|âââââââ   | 130/200 [41:22<21:55, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 65%|âââââââ   | 130/200 [41:20<21:55, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|âââââââ   | 131/200 [41:40<21:36, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|âââââââ   | 131/200 [41:39<21:36, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|âââââââ   | 132/200 [41:59<21:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|âââââââ   | 132/200 [41:58<21:17, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 66%|âââââââ   | 133/200 [42:18<20:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 66%|âââââââ   | 133/200 [42:17<20:59, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 67%|âââââââ   | 134/200 [42:37<20:39, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 67%|âââââââ   | 134/200 [42:35<20:39, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|âââââââ   | 135/200 [42:55<20:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|âââââââ   | 135/200 [42:54<20:21, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|âââââââ   | 136/200 [43:14<20:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|âââââââ   | 136/200 [43:13<20:02, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 68%|âââââââ   | 137/200 [43:33<19:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 68%|âââââââ   | 137/200 [43:32<19:43, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 69%|âââââââ   | 138/200 [43:52<19:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 69%|âââââââ   | 138/200 [43:51<19:25, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|âââââââ   | 139/200 [44:11<19:06, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|âââââââ   | 139/200 [44:09<19:06, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|âââââââ   | 140/200 [44:29<18:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|âââââââ   | 140/200 [44:28<18:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 70%|âââââââ   | 141/200 [44:48<18:28, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 70%|âââââââ   | 141/200 [44:47<18:28, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 71%|âââââââ   | 142/200 [45:07<18:10, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 71%|âââââââ   | 142/200 [45:06<18:10, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|ââââââââ  | 143/200 [45:26<17:51, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|ââââââââ  | 143/200 [45:25<17:51, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|ââââââââ  | 144/200 [45:45<17:32, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|ââââââââ  | 144/200 [45:43<17:32, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 72%|ââââââââ  | 145/200 [46:03<17:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 72%|ââââââââ  | 145/200 [46:02<17:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 73%|ââââââââ  | 146/200 [46:22<16:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 73%|ââââââââ  | 146/200 [46:21<16:55, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|ââââââââ  | 147/200 [46:41<16:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|ââââââââ  | 147/200 [46:40<16:36, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|ââââââââ  | 148/200 [47:00<16:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|ââââââââ  | 148/200 [46:59<16:17, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 74%|ââââââââ  | 149/200 [47:19<15:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 74%|ââââââââ  | 149/200 [47:17<15:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 75%|ââââââââ  | 150/200 [47:38<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 75%|ââââââââ  | 150/200 [47:36<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.2894, 'learning_rate': 3e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015                                                 #015[1,mpirank:0,algo-1]<stderr>:#015 75%|ââââââââ  | 150/200 [47:36<15:41, 18.83s/it][1,mpirank:8,algo-2]<stderr>:#015 75%|ââââââââ  | 150/200 [47:38<15:41, 18.83s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.2894, 'learning_rate': 3e-06, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|ââââââââ  | 151/200 [47:56<15:22, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|ââââââââ  | 151/200 [47:55<15:22, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|ââââââââ  | 152/200 [48:15<15:03, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|ââââââââ  | 152/200 [48:14<15:03, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 76%|ââââââââ  | 153/200 [48:34<14:44, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 76%|ââââââââ  | 153/200 [48:33<14:44, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 77%|ââââââââ  | 154/200 [48:53<14:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 77%|ââââââââ  | 154/200 [48:51<14:25, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|ââââââââ  | 155/200 [49:12<14:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|ââââââââ  | 155/200 [49:10<14:06, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|ââââââââ  | 156/200 [49:30<13:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|ââââââââ  | 156/200 [49:29<13:47, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 78%|ââââââââ  | 157/200 [49:49<13:28, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 78%|ââââââââ  | 157/200 [49:48<13:28, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 79%|ââââââââ  | 158/200 [50:08<13:10, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 79%|ââââââââ  | 158/200 [50:07<13:10, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|ââââââââ  | 159/200 [50:27<12:51, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|ââââââââ  | 159/200 [50:26<12:51, 18.82s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|ââââââââ  | 160/200 [50:46<12:32, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|ââââââââ  | 160/200 [50:44<12:32, 18.81s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 80%|ââââââââ  | 161/200 [51:04<12:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 80%|ââââââââ  | 161/200 [51:03<12:13, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 81%|ââââââââ  | 162/200 [51:23<11:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 81%|ââââââââ  | 162/200 [51:22<11:54, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|âââââââââ | 163/200 [51:42<11:35, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|âââââââââ | 163/200 [51:41<11:35, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|âââââââââ | 164/200 [52:01<11:16, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|âââââââââ | 164/200 [52:00<11:16, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 82%|âââââââââ | 165/200 [52:20<10:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 82%|âââââââââ | 165/200 [52:18<10:58, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 83%|âââââââââ | 166/200 [52:38<10:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 83%|âââââââââ | 166/200 [52:37<10:39, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|âââââââââ | 167/200 [52:57<10:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|âââââââââ | 167/200 [52:56<10:20, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|âââââââââ | 168/200 [53:16<10:01, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|âââââââââ | 168/200 [53:15<10:01, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 84%|âââââââââ | 169/200 [53:35<09:42, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 84%|âââââââââ | 169/200 [53:33<09:42, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 85%|âââââââââ | 170/200 [53:53<09:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 85%|âââââââââ | 170/200 [53:52<09:23, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|âââââââââ | 171/200 [54:12<09:04, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|âââââââââ | 171/200 [54:11<09:04, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|âââââââââ | 172/200 [54:31<08:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|âââââââââ | 172/200 [54:30<08:46, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 86%|âââââââââ | 173/200 [54:50<08:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 86%|âââââââââ | 173/200 [54:49<08:27, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 87%|âââââââââ | 174/200 [55:09<08:08, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 87%|âââââââââ | 174/200 [55:07<08:08, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|âââââââââ | 175/200 [55:27<07:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|âââââââââ | 175/200 [55:26<07:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|âââââââââ | 176/200 [55:46<07:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|âââââââââ | 176/200 [55:45<07:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 88%|âââââââââ | 177/200 [56:05<07:12, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 88%|âââââââââ | 177/200 [56:04<07:12, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 89%|âââââââââ | 178/200 [56:24<06:53, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 89%|âââââââââ | 178/200 [56:23<06:53, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|âââââââââ | 179/200 [56:43<06:34, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|âââââââââ | 179/200 [56:41<06:34, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|âââââââââ | 180/200 [57:01<06:15, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|âââââââââ | 180/200 [57:00<06:15, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 90%|âââââââââ | 181/200 [57:20<05:57, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 90%|âââââââââ | 181/200 [57:19<05:57, 18.80s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 91%|âââââââââ | 182/200 [57:39<05:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 91%|âââââââââ | 182/200 [57:38<05:38, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|ââââââââââ| 183/200 [57:58<05:19, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|ââââââââââ| 183/200 [57:57<05:19, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|ââââââââââ| 184/200 [58:17<05:00, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|ââââââââââ| 184/200 [58:15<05:00, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 92%|ââââââââââ| 185/200 [58:35<04:41, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 92%|ââââââââââ| 185/200 [58:34<04:41, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 93%|ââââââââââ| 186/200 [58:54<04:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 93%|ââââââââââ| 186/200 [58:53<04:22, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|ââââââââââ| 187/200 [59:13<04:04, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|ââââââââââ| 187/200 [59:12<04:04, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|ââââââââââ| 188/200 [59:32<03:45, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|ââââââââââ| 188/200 [59:30<03:45, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 94%|ââââââââââ| 189/200 [59:50<03:26, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 94%|ââââââââââ| 189/200 [59:49<03:26, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 95%|ââââââââââ| 190/200 [1:00:09<03:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 95%|ââââââââââ| 190/200 [1:00:08<03:07, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|ââââââââââ| 191/200 [1:00:28<02:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|ââââââââââ| 191/200 [1:00:27<02:49, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|ââââââââââ| 192/200 [1:00:47<02:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|ââââââââââ| 192/200 [1:00:46<02:30, 18.79s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 96%|ââââââââââ| 193/200 [1:01:06<02:11, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 96%|ââââââââââ| 193/200 [1:01:04<02:11, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 97%|ââââââââââ| 194/200 [1:01:24<01:52, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 97%|ââââââââââ| 194/200 [1:01:23<01:52, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|ââââââââââ| 195/200 [1:01:43<01:33, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|ââââââââââ| 195/200 [1:01:42<01:33, 18.78s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|ââââââââââ| 196/200 [1:02:02<01:15, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|ââââââââââ| 196/200 [1:02:01<01:15, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 98%|ââââââââââ| 197/200 [1:02:21<00:56, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 98%|ââââââââââ| 197/200 [1:02:19<00:56, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015 99%|ââââââââââ| 198/200 [1:02:39<00:37, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015 99%|ââââââââââ| 198/200 [1:02:38<00:37, 18.77s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|ââââââââââ| 199/200 [1:02:58<00:18, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|ââââââââââ| 199/200 [1:02:57<00:18, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'loss': 0.951, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'loss': 0.951, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:{'train_runtime': 3796.2397, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.053, 'train_loss': 1.4463429260253906, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:16<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:{'train_runtime': 3797.4775, 'train_samples_per_second': 0.421, 'train_steps_per_second': 0.053, 'train_loss': 1.4463429260253906, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:17<00:00, 18.76s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:16<00:00, 18.98s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stderr>:#015100%|ââââââââââ| 200/200 [1:03:17<00:00, 18.99s/it]\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 04:43:39.361: I smdistributed/modelparallel/torch/model.py:907] [14] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:14,algo-2]<stdout>:[2023-05-21 04:43:39.361: W smdistributed/modelparallel/torch/model.py:917] [14] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 04:43:39.365: I smdistributed/modelparallel/torch/model.py:907] [3] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:3,algo-1]<stdout>:[2023-05-21 04:43:39.365: W smdistributed/modelparallel/torch/model.py:917] [3] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 04:43:39.367: I smdistributed/modelparallel/torch/model.py:907] [4] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:4,algo-1]<stdout>:[2023-05-21 04:43:39.367: W smdistributed/modelparallel/torch/model.py:917] [4] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 04:43:39.368: I smdistributed/modelparallel/torch/model.py:907] [5] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:5,algo-1]<stdout>:[2023-05-21 04:43:39.368: W smdistributed/modelparallel/torch/model.py:917] [5] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 04:43:39.369: I smdistributed/modelparallel/torch/model.py:907] [9] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:9,algo-2]<stdout>:[2023-05-21 04:43:39.370: W smdistributed/modelparallel/torch/model.py:917] [9] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 04:43:39.373: I smdistributed/modelparallel/torch/model.py:907] [6] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:6,algo-1]<stdout>:[2023-05-21 04:43:39.373: W smdistributed/modelparallel/torch/model.py:917] [6] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 04:43:39.380: I smdistributed/modelparallel/torch/model.py:907] [13] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:13,algo-2]<stdout>:[2023-05-21 04:43:39.380: W smdistributed/modelparallel/torch/model.py:917] [13] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 04:43:39.381: I smdistributed/modelparallel/torch/model.py:907] [12] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:12,algo-2]<stdout>:[2023-05-21 04:43:39.382: W smdistributed/modelparallel/torch/model.py:917] [12] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 04:43:39.385: I smdistributed/modelparallel/torch/model.py:907] [11] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:11,algo-2]<stdout>:[2023-05-21 04:43:39.385: W smdistributed/modelparallel/torch/model.py:917] [11] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 04:43:39.385: I smdistributed/modelparallel/torch/model.py:907] [15] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:15,algo-2]<stdout>:[2023-05-21 04:43:39.385: W smdistributed/modelparallel/torch/model.py:917] [15] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 04:43:39.393: I smdistributed/modelparallel/torch/model.py:907] [2] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:2,algo-1]<stdout>:[2023-05-21 04:43:39.393: W smdistributed/modelparallel/torch/model.py:917] [2] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 04:43:39.402: I smdistributed/modelparallel/torch/model.py:907] [7] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:7,algo-1]<stdout>:[2023-05-21 04:43:39.403: W smdistributed/modelparallel/torch/model.py:917] [7] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 04:43:39.411: I smdistributed/modelparallel/torch/model.py:907] [8] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:8,algo-2]<stdout>:[2023-05-21 04:43:39.411: W smdistributed/modelparallel/torch/model.py:917] [8] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 04:43:39.419: I smdistributed/modelparallel/torch/model.py:907] [10] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:10,algo-2]<stdout>:[2023-05-21 04:43:39.420: W smdistributed/modelparallel/torch/model.py:917] [10] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 04:43:39.455: I smdistributed/modelparallel/torch/model.py:907] [0] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:[2023-05-21 04:43:39.455: W smdistributed/modelparallel/torch/model.py:917] [0] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 04:43:39.648: I smdistributed/modelparallel/torch/model.py:907] [1] Gathering model state_dict during saving. To prevent hangs, please ensure that model.state_dict() (where model is smp.DistributedModel wrapped model) is called on all the ranks with dp_rank() == 0\u001b[0m\n",
      "\u001b[34m[1,mpirank:1,algo-1]<stdout>:[2023-05-21 04:43:39.648: W smdistributed/modelparallel/torch/model.py:917] [1] gather_to_rank0 is set to True. Full state_dict will only be saved to rank 0 only, other ranks will have empty dicts\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/user_content.pt s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/user_content.pt\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/special_tokens_map.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/tokenizer_config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/added_tokens.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/added_tokens.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/training_args.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/training_args.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/generation_config.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model.bin.index.json s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/tokenizer.model s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/tokenizer.model\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model-00002-of-00002.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34m[1,mpirank:0,algo-1]<stdout>:cp /tmp/output/asset/pytorch_model-00001-of-00002.bin s3://sagemaker-us-east-1-514385905925/llama-smp-finetuned-052111/model/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-05-21 04:44:41,082 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-21 04:45:16 Uploading - Uploading generated training model\u001b[35m2023-05-21 04:44:41,094 sagemaker-training-toolkit INFO     Orted process exited\u001b[0m\n",
      "\u001b[35m2023-05-21 04:45:11,125 sagemaker-training-toolkit INFO     MPI process finished.\u001b[0m\n",
      "\u001b[35m2023-05-21 04:45:11,125 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-05-21 04:45:22 Completed - Training job completed\n",
      "Training seconds: 8520\n",
      "Billable seconds: 8520\n"
     ]
    }
   ],
   "source": [
    "# starting the train job with our uploaded datasets as input\n",
    "train_input_path = 's3://your_bucket/samples/datasets/1536-token-length-for-llama/train'\n",
    "test_input_path = 's3://your_bucket/samples/datasets/1536-token-length-for-llama/test'\n",
    "data = {\n",
    "    'train': train_input_path,\n",
    "    'test': test_input_path\n",
    "}\n",
    "\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "interpreter": {
   "hash": "c281c456f1b8161c8906f4af2c08ed2c40c50136979eaae69688b01f70e9f4a9"
  },
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
